To move data from an on-premises SQL Server database into Azure Blob Storage using Azure Data Factory (ADF) with a Shared Self-Hosted Integration Runtime, you need to follow several steps. Additionally, you need to consider firewall rules to ensure that the necessary communication is allowed between your on-premises environment and Azure services. Here's a high-level overview of the process:

1. Set Up a Self-Hosted Integration Runtime:

Install and configure the Azure Data Factory Self-Hosted Integration Runtime on an on-premises machine. This runtime acts as a bridge between your on-premises data sources and Azure Data Factory in the cloud. You can follow the Azure documentation to install and configure the runtime.
2. Define Linked Services:

In Azure Data Factory, create Linked Services for both your on-premises SQL Server and your Azure Blob Storage. Linked Services contain connection information and credentials required to connect to these data sources.
3. Create Datasets:

Define Datasets that represent the data structures of your on-premises SQL Server database tables and Azure Blob Storage containers. Specify the Linked Services in the Datasets to establish the connections.
4. Create Pipelines:

Create Pipelines that specify the data movement and transformation activities. Use activities like Copy Data to move data from SQL Server to Azure Blob Storage. Configure the source and sink settings in the Copy Data activity to specify the source Dataset (SQL Server) and the destination Dataset (Azure Blob Storage).
5. Set Up Firewall Rules:

To allow the Self-Hosted Integration Runtime to communicate with Azure services, you'll need to configure firewall rules on your on-premises environment. You should open outbound ports for the Integration Runtime to access Azure services. Specifically, you may need to open outbound ports for HTTPS (port 443) and any other ports required by Azure services used in your data movement.
6. Monitoring and Debugging:

Test your data movement pipeline and monitor its execution in Azure Data Factory. Azure Data Factory provides monitoring and debugging tools to help you identify and resolve any issues that may arise during data movement.
7. Scheduling and Automation:

Set up schedules or triggers for your pipelines to run automatically, depending on your data integration requirements.
8. Security and Credentials:

Ensure that the credentials and security settings used in your Linked Services are properly configured to access both the on-premises SQL Server and Azure Blob Storage.
Please note that the specifics of firewall rules and network configuration can vary based on your organization's security policies and network architecture. It's essential to work closely with your network and security teams to ensure the required connectivity between your on-premises environment and Azure services.

For the most up-to-date and detailed guidance, refer to the official Azure Data Factory documentation and consult with your organization's Azure administrators and network experts.
